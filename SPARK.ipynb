{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0a82832-21fe-43e1-b7f9-c905cec63d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "import  pyspark.sql.functions as fun \n",
    "import numpy as np\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b4476a7-3955-45c9-9f55-57799c0c99b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1710b0f-2b68-4057-9997-9a6a55a2bee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d53ce86-5804-43e2-9fb4-935d424122ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(1, 50))\n",
    "print(rdd.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd7f1996-e0e2-47d6-80c2-3db83baae079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1225\n"
     ]
    }
   ],
   "source": [
    "total = rdd.reduce(lambda x, y: x + y)\n",
    "print(total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37662e48-e9af-453c-9215-3dfd66d01cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.0\n"
     ]
    }
   ],
   "source": [
    "avg = rdd.reduce(lambda x, y: x + y) / rdd.count()\n",
    "print(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e5d5532-6d74-4317-a548-77cb27c87de4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1234b458-2bf9-4afc-bafe-1cae1c296967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.min()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bf5e73d-af95-46bc-b8b5-397eeecf9228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "364c24d9-ad37-4d6a-83c4-2b53ff80bf13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVEN numbers: 24\n",
      "ODD numbers: 25\n"
     ]
    }
   ],
   "source": [
    "even_count = rdd.filter(lambda x: x % 2 == 0).count()\n",
    "odd_count = rdd.filter(lambda x: x % 2 != 0).count()\n",
    "\n",
    "print(\"EVEN numbers:\", even_count)\n",
    "print(\"ODD numbers:\", odd_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d5d256c-87c6-4af2-ac67-96bc38aa178d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Nada ', 25),\n",
       " ('Mona', 30),\n",
       " ('Ahmed', 35),\n",
       " ('Khaled', 40),\n",
       " ('Ahmed', 35),\n",
       " ('Nada ', 25)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_data = [(\"Nada \", 25), (\"Mona\", 30), (\"Ahmed\", 35), (\"Khaled\", 40),(\"Ahmed\", 35), ('Nada ', 25)]\n",
    "rdd_people = sc.parallelize(people_data)\n",
    "rdd_people.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1488bf30-24c9-481c-bbb6-1ffa77c7e864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Khaled', 40]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_data =  (\"Khaled\", 40)\n",
    "rdd_people = sc.parallelize(people_data)\n",
    "rdd_people.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd9dd377-ed77-408f-a000-4df8e69fb47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.666666666666668\n"
     ]
    }
   ],
   "source": [
    "people_data = [(\"Nada\", 25), (\"Mona\", 30), (\"Ahmed\", 35), (\"Khaled\", 40), (\"Ahmed\", 35), (\"Nada\", 25)]\n",
    "rdd_people = sc.parallelize(people_data)\n",
    "ages_rdd = rdd_people.map(lambda x: x[1])\n",
    "average_age = ages_rdd.mean()\n",
    "print(average_age)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "01e695a6-400a-4303-b449-d2b5436abf1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(40, ['Khaled']),\n",
       " (25, ['Nada', 'Nada']),\n",
       " (30, ['Mona']),\n",
       " (35, ['Ahmed', 'Ahmed'])]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_data = [(\"Nada\", 25), (\"Mona\", 30), (\"Ahmed\", 35), (\"Khaled\", 40), (\"Ahmed\", 35), (\"Nada\", 25)]\n",
    "rdd_people = sc.parallelize(people_data)\n",
    "grouped_by_age = rdd_people.map(lambda x: (x[1], x[0])).groupByKey().mapValues(list)\n",
    "grouped_by_age.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b61d6380-b8ae-4b5f-81cf-9acfa66b5f9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Russia is the largest country in the world by land area',\n",
       " 'Moscow is the capital city of Russia',\n",
       " 'The Russian language is one of the most widely spoken languages in the world',\n",
       " 'Russia is known for its rich history and culture',\n",
       " 'The Trans-Siberian Railway is the longest railway line in the world',\n",
       " 'Russia has a strong tradition in literature, music and ballet',\n",
       " 'The country is famous for its cold winters and vast landscapes',\n",
       " 'Russia is a major player in global energy production']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_russia = sc.textFile(\"/data/russia.txt\")\n",
    "rdd_russia.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "312c2600-fbbc-47d8-b910-9218dd3e7069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "total_lines = rdd_russia.count()\n",
    "print(total_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d5f2dd11-8a1d-4b44-976d-4137355c2f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "lines_with_russia = rdd_russia.filter(lambda line: \"Russia\" in line).count()\n",
    "print(lines_with_russia)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4342b9bc-1a6a-4c62-a018-df54b05353b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('is', 7), ('the', 7), ('Russia', 5), ('in', 5), ('world', 3)]\n"
     ]
    }
   ],
   "source": [
    "words_rdd = rdd_russia.flatMap(lambda line: line.split())\n",
    "word_counts = words_rdd.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
    "top5_words = word_counts.takeOrdered(5, key=lambda x: -x[1])\n",
    "print(top5_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "168679c4-20be-4b62-8e9e-3acff678b397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['russia', 'is', 'the', 'largest', 'country', 'in', 'the', 'world', 'by', 'land', 'area', 'moscow', 'is', 'the', 'capital', 'city', 'of', 'russia', 'the', 'russian', 'language', 'is', 'one', 'of', 'the', 'most', 'widely', 'spoken', 'languages', 'in', 'the', 'world', 'russia', 'is', 'known', 'for', 'its', 'rich', 'history', 'and', 'culture', 'the', 'trans', 'siberian', 'railway', 'is', 'the', 'longest', 'railway', 'line', 'in', 'the', 'world', 'russia', 'has', 'a', 'strong', 'tradition', 'in', 'literature', 'music', 'and', 'ballet', 'the', 'country', 'is', 'famous', 'for', 'its', 'cold', 'winters', 'and', 'vast', 'landscapes', 'russia', 'is', 'a', 'major', 'player', 'in', 'global', 'energy', 'production']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "words_rdd = rdd_russia.flatMap(lambda line: re.findall(r'\\b\\w+\\b', line.lower()))\n",
    "print(words_rdd.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a3c7007d-dfb0-4ca5-8483-336c1f23dcfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['russia', 'largest', 'country', 'world', 'by', 'land', 'area', 'moscow', 'capital', 'city', 'russia', 'russian', 'language', 'one', 'most', 'widely', 'spoken', 'languages', 'world', 'russia', 'known', 'for', 'its', 'rich', 'history', 'and', 'culture', 'trans-siberian', 'railway', 'longest', 'railway', 'line', 'world', 'russia', 'has', 'strong', 'tradition', 'literature,', 'music', 'and', 'ballet', 'country', 'famous', 'for', 'its', 'cold', 'winters', 'and', 'vast', 'landscapes', 'russia', 'major', 'player', 'global', 'energy', 'production']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stopwords = {\"a\", \"the\", \"is\", \"to\", \"in\", \"of\"}\n",
    "\n",
    "filtered_words = [word for word in words if word not in stopwords]\n",
    "print(filtered_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "70ed6127-3071-4bb5-97d8-68d98187e309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('russia', 5), ('largest', 1), ('country', 2), ('world', 3), ('by', 1), ('land', 1), ('area', 1), ('moscow', 1), ('capital', 1), ('city', 1), ('russian', 1), ('language', 1), ('one', 1), ('most', 1), ('widely', 1), ('spoken', 1), ('languages', 1), ('known', 1), ('for', 2), ('its', 2), ('rich', 1), ('history', 1), ('and', 3), ('culture', 1), ('trans-siberian', 1), ('railway', 2), ('longest', 1), ('line', 1), ('has', 1), ('strong', 1), ('tradition', 1), ('literature,', 1), ('music', 1), ('ballet', 1), ('famous', 1), ('cold', 1), ('winters', 1), ('vast', 1), ('landscapes', 1), ('major', 1), ('player', 1), ('global', 1), ('energy', 1), ('production', 1)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "stopwords = {\"a\", \"the\", \"is\", \"to\", \"in\", \"of\"}\n",
    "filtered_words = [word for word in words if word not in stopwords]\n",
    "word_counts = Counter(filtered_words)\n",
    "word_counts_list = list(word_counts.items())\n",
    "print(word_counts_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e7687219-b033-4e28-9502-59533ebb4a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = 'id integer, name string, age integer, salary integer' \n",
    "data = [\n",
    "    (1, \"Ali\", 25, 4000),\n",
    "    (2, \"Mariam\", 30, 6000),\n",
    "    (3, \"Omar\", 35, 7000),\n",
    "    (4, \"Sara\", 28, 5000),\n",
    "    (5, \"Omar\", 25, 6500),\n",
    "    (6, \"Mariam\", 26, 7500)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data,schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b3264468-e19f-4d61-92c0-5e7ac24a6797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "+---+------+---+------+\n",
      "| id|  name|age|salary|\n",
      "+---+------+---+------+\n",
      "|  1|   Ali| 25|  4000|\n",
      "|  2|Mariam| 30|  6000|\n",
      "+---+------+---+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = 'id integer, name string, age integer, salary integer' \n",
    "data = [\n",
    "    (1, \"Ali\", 25, 4000),\n",
    "    (2, \"Mariam\", 30, 6000),\n",
    "    (3, \"Omar\", 35, 7000),\n",
    "    (4, \"Sara\", 28, 5000),\n",
    "    (5, \"Omar\", 25, 6500),\n",
    "    (6, \"Mariam\", 26, 7500)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.printSchema()\n",
    "df.show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2c24f82d-ae63-49c1-b26a-572642a76588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|  name|salary|\n",
      "+------+------+\n",
      "|   Ali|  4000|\n",
      "|Mariam|  6000|\n",
      "|  Omar|  7000|\n",
      "|  Sara|  5000|\n",
      "|  Omar|  6500|\n",
      "|Mariam|  7500|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"name\", \"salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a6f21f4f-29ca-4834-b016-1004a0989d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|avg(salary)|\n",
      "+-----------+\n",
      "|     6000.0|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "df.select(avg(\"salary\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "613888b5-86a2-4b31-946b-02db31f170ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+------+\n",
      "| id|  name|age|salary|\n",
      "+---+------+---+------+\n",
      "|  2|Mariam| 30|  6000|\n",
      "|  3|  Omar| 35|  7000|\n",
      "+---+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.age > 28).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7cb2626f-912f-4220-b8c3-07f2cfc91d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"name\").distinct().count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a9cce5ff-d8d5-48f5-99ef-f555722f926e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "|  name|avg(salary)|\n",
      "+------+-----------+\n",
      "|   Ali|     4000.0|\n",
      "|Mariam|     6750.0|\n",
      "|  Omar|     6750.0|\n",
      "|  Sara|     5000.0|\n",
      "+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "df.groupBy(\"name\").avg(\"salary\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8c0e3406-49b1-461a-8e04-3497bce3f72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+\n",
      "|  name|average_salary|\n",
      "+------+--------------+\n",
      "|   Ali|        4000.0|\n",
      "|Mariam|        6750.0|\n",
      "|  Omar|        6750.0|\n",
      "|  Sara|        5000.0|\n",
      "+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "df.groupBy(\"name\").agg(avg(\"salary\").alias(\"average_salary\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e199806-d0b2-46ef-93aa-e8e2c0028d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read NullData\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d7ce65b-2a54-4244-ad92-c8cf5029bd3b",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "java.net.URISyntaxException: Relative path in absolute URI: C:%5CUsers%5CKhashaba%5CDesktop%5CData%20Engingeer%5CSpark%5Cshared%5CNullData.csv",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df1 \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mKhashaba\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDesktop\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mData Engingeer\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mSpark\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mshared\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mNullData.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minferSchema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m df1\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:740\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: java.net.URISyntaxException: Relative path in absolute URI: C:%5CUsers%5CKhashaba%5CDesktop%5CData%20Engingeer%5CSpark%5Cshared%5CNullData.csv"
     ]
    }
   ],
   "source": [
    "df1 = spark.read.csv(r\"C:\\Users\\Khashaba\\Desktop\\Data Engingeer\\Spark\\shared\\NullData.csv\",\n",
    "                     header=True, inferSchema=True)\n",
    "\n",
    "df1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494170ca-a854-4543-9d6c-cdb6c83970a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
